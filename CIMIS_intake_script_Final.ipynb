{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEGR+xEDDjQRFjHCM6DKJ/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tr3nt-tayl0r/CIMIS/blob/main/CIMIS_intake_script_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQQFxGATNjVS",
        "outputId": "ffe0744b-0671-4380-b483-ed77b13d89b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CIMIS'...\n",
            "remote: Enumerating objects: 8857, done.\u001b[K\n",
            "remote: Counting objects: 100% (2822/2822), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2791/2791), done.\u001b[K\n",
            "remote: Total 8857 (delta 78), reused 2715 (delta 29), pack-reused 6035\u001b[K\n",
            "Receiving objects: 100% (8857/8857), 273.97 MiB | 8.01 MiB/s, done.\n",
            "Resolving deltas: 100% (662/662), done.\n",
            "Updating files: 100% (3632/3632), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/tr3nt-tayl0r/CIMIS.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import requests\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import userdata\n",
        "!pip install cartopy\n",
        "import cartopy\n",
        "import geopandas as gpd\n",
        "!pip install refet\n",
        "import math\n",
        "import refet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8PWYpyjeDkJ",
        "outputId": "887a7a0c-5b87-46b6-a94f-bc33e7888b27",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cartopy\n",
            "  Downloading Cartopy-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from cartopy) (1.25.2)\n",
            "Requirement already satisfied: matplotlib>=3.5 in /usr/local/lib/python3.10/dist-packages (from cartopy) (3.7.1)\n",
            "Requirement already satisfied: shapely>=1.7 in /usr/local/lib/python3.10/dist-packages (from cartopy) (2.0.4)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.10/dist-packages (from cartopy) (24.1)\n",
            "Requirement already satisfied: pyshp>=2.3 in /usr/local/lib/python3.10/dist-packages (from cartopy) (2.3.1)\n",
            "Requirement already satisfied: pyproj>=3.3.1 in /usr/local/lib/python3.10/dist-packages (from cartopy) (3.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5->cartopy) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5->cartopy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5->cartopy) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5->cartopy) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5->cartopy) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5->cartopy) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5->cartopy) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pyproj>=3.3.1->cartopy) (2024.6.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.5->cartopy) (1.16.0)\n",
            "Installing collected packages: cartopy\n",
            "Successfully installed cartopy-0.23.0\n",
            "Collecting refet\n",
            "  Downloading refet-0.4.2.tar.gz (27 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from refet) (1.25.2)\n",
            "Building wheels for collected packages: refet\n",
            "  Building wheel for refet (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for refet: filename=refet-0.4.2-py3-none-any.whl size=19273 sha256=087c5c2e724025b5dc191022a45de6f1e0395e8c0f725f7a2878d358a1594681\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/55/da/7ed6e47f6fa1135fcdef373db6f974fd7515ee8e86c9662e88\n",
            "Successfully built refet\n",
            "Installing collected packages: refet\n",
            "Successfully installed refet-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell contains functions that are used in multiple places throughout the notebook. It contains a dictionary of column names. It also performs the temperature corrections that are used to generate the temperature-corrected files."
      ],
      "metadata": {
        "id": "B2rA5Fn4hzdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_col_names(df):\n",
        "  '''corrects column names from CIMIS based on dictionary'''\n",
        "  rename_dict = {'DayAirTmpMin.Value':'Tmin',\n",
        "               'DayAirTmpMin.Qc':'Tmin_Qc',\n",
        "               'DayAirTmpMax.Value':'Tmax',\n",
        "               'DayAirTmpMax.Qc':'Tmax_Qc',\n",
        "               'DayDewPnt.Value':'Tdew',\n",
        "               'DayDewPnt.Qc':'Tdew_Qc',\n",
        "               'DayAirTmpAvg.Value':'Tavg',\n",
        "               'DayAirTmpAvg.Qc':'Tavg_Qc',\n",
        "               'DayEto.Value':'ETo',\n",
        "               'DayEto.Qc':'ETo_Qc',\n",
        "               'DayRelHumMin.Value':'RHmin',\n",
        "               'DayRelHumMin.Qc':'RHmin_Qc',\n",
        "               'DayRelHumMax.Value':'RHmax',\n",
        "               'DayRelHumMax.Qc':'RHmax_Qc',\n",
        "               'DayRelHumAvg.Value':'RHavg',\n",
        "               'DayRelHumAvg.Qc':'RHavg_Qc',\n",
        "               'DayPrecip.Value':'Pr',\n",
        "               'DayPrecip.Qc':'Pr_Qc',\n",
        "               'DaySolRadAvg.Value':'Rs',\n",
        "               'DaySolRadAvg.Qc':'Rs_Qc',\n",
        "               'DayVapPresAvg.Value':'Ea',\n",
        "               'DayVapPresAvg.Qc':'Ea_Qc',\n",
        "               'DayWindSpdAvg.Value':'u2',\n",
        "               'DayWindSpdAvg.Qc':'u2_Qc',\n",
        "               'Julian':'Doy'}\n",
        "  df.rename(columns=rename_dict, inplace=True)\n",
        "  dfout = df[['Date','Tmin','Tmax','Tdew','Tavg','ETo','Pr','RHmin','RHavg']]\n",
        "  dfout.set_index(pd.to_datetime(dfout.Date),inplace=True)\n",
        "  return dfout\n",
        "\n",
        "def corr_nref(df,tminNRef='Tmin',tdewNRef='Tdew',tmaxNRef='Tmax',bT=True):\n",
        "    df['dT']=df[tminNRef]-df[tdewNRef]\n",
        "    if bT==True:\n",
        "        df['bT']=0.3\n",
        "    dfAI = df[['Pr','ETo']].dropna(axis=0)\n",
        "    AI = dfAI['Pr'].mean()/dfAI['ETo'].mean()\n",
        "    print('\\t\\tAI = '+str(np.round(AI,2)))\n",
        "\n",
        "    if AI < 0.05:\n",
        "        aT=5\n",
        "    elif (AI >= 0.05) & (AI < 0.2):\n",
        "        aT=2.5\n",
        "    elif (AI >= 0.2) & (AI < 0.5):\n",
        "        aT=1.5\n",
        "    elif (AI >= 0.5) & (AI < 0.65):\n",
        "        aT=0.5\n",
        "    else:\n",
        "        aT=0\n",
        "\n",
        "    df['aT']=aT\n",
        "\n",
        "    #creates the columns for corrections, so that the keys exist for the next step\n",
        "    df['Tmax_corr'] = 0\n",
        "    df['Tmin_corr'] = 0\n",
        "    df['Tdew_corr'] = 0\n",
        "\n",
        "    # Might need to add a line only to correct if dT > aT from the logic statements above\n",
        "    for index, row in df.iterrows():\n",
        "      if row['dT'] > aT:\n",
        "        max_corr = row['Tmax'] - (row['bT']*(row['dT']-row['aT'])) #(2.13)\n",
        "        df.loc[index, 'Tmax_corr'] = max_corr\n",
        "        min_corr = row['Tmin'] - (row['bT']*(row['dT']-row['aT'])) #(2.14)\n",
        "        df.loc[index, 'Tmin_corr'] = min_corr\n",
        "        dew_corr = row['Tdew'] + ((1.0-row['bT'])*(row['dT']-row['aT'])) #(2.15)\n",
        "        df.loc[index, 'Tdew_corr'] = dew_corr\n",
        "      else:\n",
        "        max = row['Tmax']\n",
        "        df.loc[index, 'Tmax_corr'] = max\n",
        "        min = row['Tmin']\n",
        "        df.loc[index, 'Tmin_corr'] = min\n",
        "        dew = row['Tdew']\n",
        "        df.loc[index, 'Tdew_corr'] = dew\n",
        "    df['AI']= AI\n",
        "    # df.set_index(df.Date,inplace=True)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "qbT3HmyReNrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell creates a csv file with data about each station such as latitude, longitude & elevation. This file is used in later cells."
      ],
      "metadata": {
        "id": "xP7ANWo0fsUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# http://et.water.ca.gov/api/station\n",
        "\n",
        "station_url = 'http://et.water.ca.gov/api/station'\n",
        "res = requests.get(station_url)\n",
        "response = json.loads(res.text)\n",
        "payload = response['Stations']\n",
        "df = pd.json_normalize(payload)\n",
        "\n",
        "rename_dict = {'HmsLatitude':'Lat',\n",
        "               'HmsLongitude':'Long'}\n",
        "df.rename(columns=rename_dict, inplace=True)\n",
        "\n",
        "# -120º6'46W / -120.112910\n",
        "def split_and_convert(row):\n",
        "  return float(row.split(' / ')[1].strip())\n",
        "\n",
        "df['Lat'] = df['Lat'].apply(split_and_convert)\n",
        "df['Long'] = df['Long'].apply(split_and_convert)\n",
        "\n",
        "if os.getcwd() != '/content/CIMIS':\n",
        "  os.chdir(\"CIMIS\")\n",
        "if not os.path.exists(\"./stations\"):\n",
        "  os.mkdir(\"./stations\")\n",
        "\n",
        "dir = '/content/CIMIS/stations/'\n",
        "file = f'{dir}stations.csv'\n",
        "df.to_csv(file, index=False)\n"
      ],
      "metadata": {
        "id": "jVy4uz8CfbhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell obtains raw data from the CIMIS API. Because there is a limit from CIMIS of 1750 records at a time, this function gets data from the API one year at a time. It is organized such that each station has its own folder, and each file within the folder is named with the station number and the year in the file name. These files are fed into a later cell to concatenate them into one MASTER file for each station, which allows to calculate the long-term aridity index.\n",
        "\n",
        "Station 125 stops downlaoding data from the API at year 2012 for some reason, so it is sometimes necessary to restart the loop from after station 125."
      ],
      "metadata": {
        "id": "4bRTHrJoe9hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.getcwd())\n",
        "if os.getcwd() != '/content/CIMIS':\n",
        "  os.chdir(\"CIMIS\")\n",
        "dir = \"./CIMIS_Project_Data/CIMIS_daily_raw_data/\"\n",
        "\n",
        "cimis_api = \"http://et.water.ca.gov/api\"\n",
        "api_key = userdata.get('cimis_key')\n",
        "token = userdata.get('github_token')\n",
        "\n",
        "# list of possible data items found here: https://et.water.ca.gov/Rest/Index\n",
        "data_items = '''day-eto,day-precip,day-sol-rad-avg,day-vap-pres-avg,day-air-tmp-max,day-air-tmp-min,day-air-tmp-avg,day-rel-hum-max,day-rel-hum-min,day-rel-hum-avg,day-dew-pnt,day-wind-spd-avg,day-wind-run,day-soil-tmp-avg'''\n",
        "\n",
        "#station_ids = [2, 5, 6, 7, 12, 13, 15, 35, 39, 41, 43, 44, 47, 52, 62, 64, 68, 70, 71, 75, 77, 78, 80, 83, 84, 87, 88, 90, 91, 99, 103, 104, 105, 106, 107, 113, 114, 116, 117, 124, 125, 126, 129, 131, 135, 136, 139, 140, 143, 144, 146, 147, 148, 150, 151, 152, 153, 157, 158, 159, 160, 163, 165, 170, 171, 173, 174, 175, 178, 179, 181, 182, 183, 184, 187, 189, 191, 192, 193, 194, 195, 197, 199, 200, 202, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 231, 232, 233, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268]\n",
        "station_ids = [2]\n",
        "\n",
        "#it may be necessary to manually get stations 125, 143, 208, & 251, because the API seems to get hung up on these stations\n",
        "#station_ids = [125, 143, 208, 251]\n",
        "\n",
        "for id in station_ids:\n",
        "  if not os.path.exists(f'{dir}station{id}'):\n",
        "    os.makedirs(f'{dir}station{id}')\n",
        "  for year in range(2003,2024):\n",
        "    start_date = f'{year}-01-01'\n",
        "    end_date = f'{year}-12-31'\n",
        "    rest_url = f'{cimis_api}/data?appKey={api_key}&targets={id}&startDate={start_date}&endDate={end_date}&dataItems={data_items}&unitOfMeasure=M'\n",
        "    res = requests.get(rest_url)\n",
        "    response = json.loads(res.text)\n",
        "    payload = response['Data']['Providers'][0]['Records']\n",
        "    df = pd.json_normalize(payload)\n",
        "    df = df[df.columns.drop(list(df.filter(regex='Unit')))]\n",
        "    filename = f'{dir}station{id}/station_id{id}_cimis_daily_raw{year}.csv'\n",
        "    print(filename)\n",
        "    df.to_csv(filename, index=False)\n",
        "  today = pd.Timestamp.now().date()\n",
        "  start_date = f'2024-01-01'\n",
        "  end_date = f'{today}'\n",
        "  rest_url = f'{cimis_api}/data?appKey={api_key}&targets={id}&startDate={start_date}&endDate={end_date}&dataItems={data_items}&unitOfMeasure=M'\n",
        "  res = requests.get(rest_url)\n",
        "  response = json.loads(res.text)\n",
        "  payload = response['Data']['Providers'][0]['Records']\n",
        "  df = pd.json_normalize(payload)\n",
        "  df = df[df.columns.drop(list(df.filter(regex='Unit')))]\n",
        "\n",
        "  filename = f'{dir}station{id}/station_id{id}_cimis_daily_raw2024.csv'\n",
        "  print(filename)\n",
        "  df.to_csv(filename, index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dkDcYINzeRiJ",
        "outputId": "d6c7f46b-39ae-467c-f5fb-e7dd948d03b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CIMIS\n",
            "./CIMIS_Project_Data/CIMIS_daily_raw_data/station2/station_id2_cimis_daily_raw2003.csv\n",
            "./CIMIS_Project_Data/CIMIS_daily_raw_data/station2/station_id2_cimis_daily_raw2004.csv\n",
            "./CIMIS_Project_Data/CIMIS_daily_raw_data/station2/station_id2_cimis_daily_raw2005.csv\n",
            "./CIMIS_Project_Data/CIMIS_daily_raw_data/station2/station_id2_cimis_daily_raw2006.csv\n",
            "./CIMIS_Project_Data/CIMIS_daily_raw_data/station2/station_id2_cimis_daily_raw2007.csv\n",
            "./CIMIS_Project_Data/CIMIS_daily_raw_data/station2/station_id2_cimis_daily_raw2008.csv\n",
            "./CIMIS_Project_Data/CIMIS_daily_raw_data/station2/station_id2_cimis_daily_raw2009.csv\n",
            "./CIMIS_Project_Data/CIMIS_daily_raw_data/station2/station_id2_cimis_daily_raw2010.csv\n",
            "./CIMIS_Project_Data/CIMIS_daily_raw_data/station2/station_id2_cimis_daily_raw2011.csv\n",
            "./CIMIS_Project_Data/CIMIS_daily_raw_data/station2/station_id2_cimis_daily_raw2012.csv\n",
            "./CIMIS_Project_Data/CIMIS_daily_raw_data/station2/station_id2_cimis_daily_raw2013.csv\n",
            "./CIMIS_Project_Data/CIMIS_daily_raw_data/station2/station_id2_cimis_daily_raw2014.csv\n",
            "./CIMIS_Project_Data/CIMIS_daily_raw_data/station2/station_id2_cimis_daily_raw2015.csv\n",
            "./CIMIS_Project_Data/CIMIS_daily_raw_data/station2/station_id2_cimis_daily_raw2016.csv\n",
            "./CIMIS_Project_Data/CIMIS_daily_raw_data/station2/station_id2_cimis_daily_raw2017.csv\n",
            "./CIMIS_Project_Data/CIMIS_daily_raw_data/station2/station_id2_cimis_daily_raw2018.csv\n",
            "./CIMIS_Project_Data/CIMIS_daily_raw_data/station2/station_id2_cimis_daily_raw2019.csv\n",
            "./CIMIS_Project_Data/CIMIS_daily_raw_data/station2/station_id2_cimis_daily_raw2020.csv\n",
            "./CIMIS_Project_Data/CIMIS_daily_raw_data/station2/station_id2_cimis_daily_raw2021.csv\n",
            "./CIMIS_Project_Data/CIMIS_daily_raw_data/station2/station_id2_cimis_daily_raw2022.csv\n",
            "./CIMIS_Project_Data/CIMIS_daily_raw_data/station2/station_id2_cimis_daily_raw2023.csv\n",
            "./CIMIS_Project_Data/CIMIS_daily_raw_data/station2/station_id2_cimis_daily_raw2024.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell modifies the raw data files so that they include information about the station, such as latitude, longitude, and elevation."
      ],
      "metadata": {
        "id": "h6KirSw31CqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "station_ids = [2, 5, 6, 7, 12, 13, 15, 35, 39, 41, 43, 44, 47, 52, 62, 64, 68, 70, 71, 75, 77, 78, 80, 83, 84, 87, 88, 90, 91, 99, 103, 104, 105, 106, 107, 113, 114, 116, 117, 124, 125, 126, 129, 131, 135, 136, 139, 140, 143, 144, 146, 147, 148, 150, 151, 152, 153, 157, 158, 159, 160, 163, 165, 170, 171, 173, 174, 175, 178, 179, 181, 182, 183, 184, 187, 189, 191, 192, 193, 194, 195, 197, 199, 200, 202, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 231, 232, 233, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268]\n",
        "#station_ids = [125, 126, 129, 131, 135, 136, 139, 140, 143, 144, 146, 147, 148, 150, 151, 152, 153, 157, 158, 159, 160, 163, 165, 170, 171, 173, 174, 175, 178, 179, 181, 182, 183, 184, 187, 189, 191, 192, 193, 194, 195, 197, 199, 200, 202, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 231, 232, 233, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268]\n",
        "#station_ids = [2]\n",
        "\n",
        "dir = \"/content/CIMIS/CIMIS_Project_Data/CIMIS_daily_raw_data/\"\n",
        "station_dir = \"/content/CIMIS/stations/\"\n",
        "\n",
        "df_stations = pd.read_csv(f'{station_dir}stations.csv')\n",
        "\n",
        "for id in station_ids:\n",
        "  for year in range(2003,2025):\n",
        "    file = f'{dir}station{id}/station_id{id}_cimis_daily_raw{year}.csv'\n",
        "    if os.path.exists(file):\n",
        "      if os.stat(file).st_size > 1:\n",
        "        df = pd.read_csv(file)\n",
        "        fix_col_names(df)\n",
        "\n",
        "        df_id = df_stations[df_stations['StationNbr'] == id]\n",
        "\n",
        "        elev = df_id['Elevation'].values[0]\n",
        "        df.loc[:,'Elev'] = elev\n",
        "\n",
        "        lat = df_id['Lat'].values[0]\n",
        "        df.loc[:,'Lat'] = lat\n",
        "\n",
        "        longtd = df_id['Long'].values[0]\n",
        "        df.loc[:,'Long'] = longtd\n",
        "\n",
        "        df.to_csv(file, index=False)\n",
        "      else:\n",
        "        continue\n",
        "    else:\n",
        "      continue\n"
      ],
      "metadata": {
        "id": "Krs6q-ja_RQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell concatenates all of the files for each respective station into one MASTER file, which allows us to compute the long-term aridity index for each station.\n",
        "\n",
        "Because this cell concatenates each new file into any already-existing file, it is necessary to delete any existing MASTER file if an update needs to be made."
      ],
      "metadata": {
        "id": "Qpol5U9-_3YD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "station_ids = [2, 5, 6, 7, 12, 13, 15, 35, 39, 41, 43, 44, 47, 52, 62, 64, 68, 70, 71, 75, 77, 78, 80, 83, 84, 87, 88, 90, 91, 99, 103, 104, 105, 106, 107, 113, 114, 116, 117, 124, 125, 126, 129, 131, 135, 136, 139, 140, 143, 144, 146, 147, 148, 150, 151, 152, 153, 157, 158, 159, 160, 163, 165, 170, 171, 173, 174, 175, 178, 179, 181, 182, 183, 184, 187, 189, 191, 192, 193, 194, 195, 197, 199, 200, 202, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 231, 232, 233, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268]\n",
        "\n",
        "dir = \"/content/CIMIS/CIMIS_Project_Data/CIMIS_daily_raw_data/\"\n",
        "for id in station_ids:\n",
        "  for year in range(2003,2025):\n",
        "    file = f'{dir}station{id}/station_id{id}_cimis_daily_raw{year}.csv'\n",
        "    #checks if file exists\n",
        "    if os.path.exists(file):\n",
        "      #checks if file is empty for years that a station was not active\n",
        "      if os.stat(file).st_size > 1:\n",
        "        df = pd.read_csv(file)\n",
        "\n",
        "        #if a file already exists, it concatenates to that file, else creates it\n",
        "        out_filename = f'/content/CIMIS/CIMIS_Project_Data/CIMIS_daily_raw_data/station{id}/station_id{id}_cimis_daily_raw_MASTER.csv'\n",
        "        if(os.path.exists(out_filename)):\n",
        "          existing_df = pd.read_csv(out_filename)\n",
        "          out_df = pd.concat([existing_df, df])\n",
        "          out_df.to_csv(out_filename, index=False)\n",
        "        else:\n",
        "          df.to_csv(out_filename, index=False)\n",
        "      else:\n",
        "        continue\n",
        "    else:\n",
        "      continue\n",
        "\n"
      ],
      "metadata": {
        "id": "ORGotJii_4bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell is used to commit the raw files to GitHub after all necessary columns have been added to the csv files."
      ],
      "metadata": {
        "id": "3GWs5yCa5NTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.getcwd() != '/content/CIMIS':\n",
        "  os.chdir(\"CIMIS\")\n",
        "token = userdata.get('github_token')\n",
        "!git --version\n",
        "!git config --global user.email \"tretaylor@csumb.edu\"\n",
        "!git config --global user.name \"tr3nt-tayl0r\"\n",
        "!git add -A\n",
        "!git commit -m \"Commiting raw files\"\n",
        "!git remote rm origin\n",
        "!git remote add origin https://tr3nt-tayl0r:{token}@github.com/tr3nt-tayl0r/CIMIS.git\n",
        "!git push --set-upstream origin main"
      ],
      "metadata": {
        "id": "YFbC64dR4azZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell takes each MASTER raw file as input in order to perform the temperature corrections and creates the corrected files. These temperature corrections are used to more accurately compute ETo on days when the difference between the min temp and the dew point temp is greater than alpha-T, which is computed from the aridity index. This cell also calculates the average monthly difference between the observed temperatures and the corrected temperatures.\n"
      ],
      "metadata": {
        "id": "adi7ab4vBCcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "station_ids = [2, 5, 6, 7, 12, 13, 15, 35, 39, 41, 43, 44, 47, 52, 62, 64, 68, 70, 71, 75, 77, 78, 80, 83, 84, 87, 88, 90, 91, 99, 103, 104, 105, 106, 107, 113, 114, 116, 117, 124, 125, 126, 129, 131, 135, 136, 139, 140, 143, 144, 146, 147, 148, 150, 151, 152, 153, 157, 158, 159, 160, 163, 165, 170, 171, 173, 174, 175, 178, 179, 181, 182, 183, 184, 187, 189, 191, 192, 193, 194, 195, 197, 199, 200, 202, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 231, 232, 233, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268]\n",
        "#station_ids = [2]\n",
        "\n",
        "for id in station_ids:\n",
        "  filename = f'/content/CIMIS/CIMIS_Project_Data/CIMIS_daily_raw_data/station{id}/station_id{id}_cimis_daily_raw_MASTER.csv'\n",
        "\n",
        "  #checks if file exists\n",
        "  if os.path.exists(filename):\n",
        "    #checks if file is empty for years that a station was not active\n",
        "    if os.stat(filename).st_size > 1:\n",
        "      df_station = pd.read_csv(filename)\n",
        "\n",
        "      #Switching 'Date' to the python datetime\n",
        "      df_station['Date']=pd.to_datetime(df_station['Date'])\n",
        "      df_station.set_index(pd.to_datetime(df_station.Date), inplace=True)\n",
        "\n",
        "      print(id)\n",
        "      df_station = corr_nref(df_station)\n",
        "\n",
        "      df_station['Tmax_diff'] = df_station['Tmax_corr'] - df_station['Tmax']\n",
        "      df_station['Tmin_diff'] = df_station['Tmin_corr'] - df_station['Tmin']\n",
        "      df_station['Tdew_diff'] = df_station['Tdew_corr'] - df_station['Tdew']\n",
        "\n",
        "      df_station['Tmax_diff_avg'] = df_station['Tmax_diff'].groupby(pd.Grouper(axis=0, freq='M')).mean()\n",
        "      df_station['Tmin_diff_avg'] = df_station['Tmin_diff'].groupby(pd.Grouper(axis=0, freq='M')).mean()\n",
        "      df_station['Tdew_diff_avg'] = df_station['Tdew_diff'].groupby(pd.Grouper(axis=0, freq='M')).mean()\n",
        "\n",
        "      #if a corrected file already exists, it concatenates to that file, else creates it\n",
        "      out_filename = f'/content/CIMIS/CIMIS_Project_Data/CIMIS_daily_corr_data/station_id{id}_cimis_daily_corr.csv'\n",
        "      if(os.path.exists(out_filename)):\n",
        "        existing_df = pd.read_csv(out_filename)\n",
        "        out_df = pd.concat([existing_df, df_station])\n",
        "        out_df.to_csv(out_filename, index=False)\n",
        "      else:\n",
        "        df_station.to_csv(out_filename, index=False)\n",
        "    else:\n",
        "      continue\n",
        "  else:\n",
        "    continue\n"
      ],
      "metadata": {
        "id": "TaK3We_dB1Dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell adds the monthly mean temperature difference to each day of a given month. In other words, when we calculate the mean monthly temperature difference for Tmin, Tmax, and Tdew in the previous step, it is initially only saved in the row associated with the last day of a month, so this cell puts that value into every row of the month."
      ],
      "metadata": {
        "id": "1LYp8MNCCRsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "station_ids = [2, 5, 6, 7, 12, 13, 15, 35, 39, 41, 43, 44, 47, 52, 62, 64, 68, 70, 71, 75, 77, 78, 80, 83, 84, 87, 88, 90, 91, 99, 103, 104, 105, 106, 107, 113, 114, 116, 117, 124, 125, 126, 129, 131, 135, 136, 139, 140, 143, 144, 146, 147, 148, 150, 151, 152, 153, 157, 158, 159, 160, 163, 165, 170, 171, 173, 174, 175, 178, 179, 181, 182, 183, 184, 187, 189, 191, 192, 193, 194, 195, 197, 199, 200, 202, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 231, 232, 233, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268]\n",
        "\n",
        "for id in station_ids:\n",
        "  file = f'/content/CIMIS/CIMIS_Project_Data/CIMIS_daily_corr_data/station_id{id}_cimis_daily_corr.csv'\n",
        "  df = pd.read_csv(file)\n",
        "  df.set_index(pd.to_datetime(df.Date, format='mixed'), inplace=True)\n",
        "  last_day = df.resample('M').last()\n",
        "  last_day.set_index(pd.to_datetime(last_day.Date, format='mixed'), inplace=True)\n",
        "  first_day = df.resample('M').first()\n",
        "  first_day.set_index(pd.to_datetime(first_day.Date, format='mixed'), inplace=True)\n",
        "  out_df = pd.DataFrame()\n",
        "  for i in range(last_day.index.size):\n",
        "    filtered_df = df.loc[first_day.index[i]:last_day.index[i]]\n",
        "    filtered_df.loc[:,'Tdew_diff_avg'] = last_day.iloc[i]['Tdew_diff_avg']\n",
        "    filtered_df.loc[:,'Tmax_diff_avg'] = last_day.iloc[i]['Tmax_diff_avg']\n",
        "    filtered_df.loc[:,'Tmin_diff_avg'] = last_day.iloc[i]['Tmin_diff_avg']\n",
        "    out_df = pd.concat([out_df, filtered_df])\n",
        "  out_df.to_csv(file, index=False)\n"
      ],
      "metadata": {
        "id": "GE5WJILsCSdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell calculates uncorrected and corrected ETo values for each day for each respective station using the RefET package and stores them in the temp-corrected file for that station."
      ],
      "metadata": {
        "id": "h8ofmrLODDLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = '/content/CIMIS/CIMIS_Project_Data/CIMIS_daily_corr_data/'\n",
        "\n",
        "units={'tmin': 'C', 'tmax': 'C', 'tdew': 'C', 'rs': 'w/m2',\n",
        "                 'uz': 'm/s', 'lat': 'deg'}\n",
        "\n",
        "def calc_et_uncorr(row):\n",
        "  eto_uncor = refet.Daily(\n",
        "    tmin=row['Tmin'], tmax=row['Tmax'], tdew=row['Tdew'], rs=float(row['Rs']), uz=row['u2'],\n",
        "    zw=2, elev=row['Elev'], lat=row['Lat'], doy=row['Doy'], method='asce',\n",
        "    input_units=units).eto()\n",
        "  return eto_uncor[0]\n",
        "\n",
        "\n",
        "def calc_et_corr(row):\n",
        "  eto_corr = refet.Daily(\n",
        "    tmin=row['Tmin_corr'], tmax=row['Tmax_corr'], tdew=row['Tdew_corr'], rs=float(row['Rs']), uz=row['u2'],\n",
        "    zw=2, elev=row['Elev'], lat=row['Lat'], doy=row['Doy'], method='asce',\n",
        "    input_units=units).eto()\n",
        "  return eto_corr[0]\n",
        "\n",
        "station_ids = [2, 5, 6, 7, 12, 13, 15, 35, 39, 41, 43, 44, 47, 52, 62, 64, 68, 70, 71, 75, 77, 78, 80, 83, 84, 87, 88, 90, 91, 99, 103, 104, 105, 106, 107, 113, 114, 116, 117, 124, 125, 126, 129, 131, 135, 136, 139, 140, 143, 144, 146, 147, 148, 150, 151, 152, 153, 157, 158, 159, 160, 163, 165, 170, 171, 173, 174, 175, 178, 179, 181, 182, 183, 184, 187, 189, 191, 192, 193, 194, 195, 197, 199, 200, 202, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 231, 232, 233, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268]\n",
        "#station_ids = [2, 5, 6]\n",
        "\n",
        "for id in station_ids:\n",
        "  file = f'{dir}station_id{id}_cimis_daily_corr.csv'\n",
        "  if os.path.exists(file):\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    df['ETo_uncor'] = df.apply(calc_et_uncorr, axis=1)\n",
        "    df['ETo_corr'] = df.apply(calc_et_corr, axis=1)\n",
        "    df['ETo_CIMIS'] = df['ETo']\n",
        "    print(\"Station number: \" + str(id))\n",
        "    df.to_csv(file, index=False)\n"
      ],
      "metadata": {
        "id": "x-fmwaWsDEGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell commits the temp-corrected files to GitHub now that all new columns have been saved to the csv files."
      ],
      "metadata": {
        "id": "yTCVrQURD1Ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token = userdata.get('github_token')\n",
        "!git --version\n",
        "!git config --global user.email \"tretaylor@csumb.edu\"\n",
        "!git config --global user.name \"tr3nt-tayl0r\"\n",
        "!git add -A\n",
        "!git commit -m \"Commiting MASTER raw files & corrected files\"\n",
        "!git remote rm origin\n",
        "!git remote add origin https://tr3nt-tayl0r:{token}@github.com/tr3nt-tayl0r/CIMIS.git\n",
        "!git push --set-upstream origin main"
      ],
      "metadata": {
        "id": "Y9Nd2JZLEFlz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}